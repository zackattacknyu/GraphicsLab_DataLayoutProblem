\section{Complexity Analysis}

We now analyze the running time and storage requirements of our algorithm. Let $N$ be the number of data units and $A$ be the number of access requirements. We will use $k$ as the average span of a single access requirement. The variable $Q$ will represent how many executions of the redundancy loop occur. The average number of overlapping access requirements is proportional \YOON{this claim is not intuitive to me. While there are many data units, it can have no overlapping?} to the number of data units multiplied by the redundancy factor $r$, which is the amount of redundancy if we have a single-seek\YOON{Did you define this term earlier?} layout. Therefore, the number of overlapping access requirements is $O(rN)$. 

{\bf Time Complexity:} The construction of the heaps $E_M$ and $E_C$ involve computing the benefit information for all $A$ access requirements and inserting each one into the heap. Computing the benefit information of moving or copying a data unit involves scanning the span of the access requirement and it takes $O(k)$ operations, while inserting this benefit information into the heap takes $O(log (A))$ operations. Thus it takes a total of $O(A (k + logA))$ operations to do the initial construction of the heaps. \\
\\
After the initial construction of the heap as above, in every iteration, element from the top of the heap is removed, processed, the benefit function is recalculated for affected access requirements, and the heap is updated. For each data unit move, $O(rN)$ overlapping access requirements are affected, and for each of these access requirements $O(k+log(A))$ is required to recalculate the benefit data and update the heap. Thus the final loop takes $O(QrN(k + logA))$ operations. \\
\\
This means that in total, our algorithm takes $O((QrN + A)(k + logA))$ operations. In all likelihood, we will have to run the loop at least once, so $Q \geq 1$. Additionally, since $r \geq 1$, we know that $rN \geq A$. Thus we can simplify the expression to say that our algorithm takes $O(QrN(k + logA))$ operations. 

\subsection{Space Complexity}

During the run of the algorithm, we have to store the number of overlapping access requirements at each data unit, which will require $O(N)$ storage. We will also have to store a heap of access requirements, which can be stored using $O(A)$ space. We also have a list of access requirements and that information will take up $O(A)$ space. In total we thus have $O(A + N)$ storage space used during the run of the algorithm. 


\subsection{Linear search justification}

In order to find the best place to copy a data unit, we perform a linear search within the span of the access requirement for location with the largest benefit. If $k$ is the span of the access requirement, then the linear search takes $O(k)$ query time. Updates will also be $O(k)$ and construction will be $O(N)$ with N being the number of data units. There are other approaches, such as a range tree or dynamic programming, that may produce better query times, but their construction and update times will be worse as well as their storage. \\
\\
With dynamic programming, we would have to maintain a matrix where an entry
$(i,j)$ would contain the minimum value in that range. This would give us a
$O(1)$ query time but the construction and storage would be $O(N^2)$ where N is
the number of data units. The update time would be $O(N)$ when we add a data
unit. Since the $N$ for this problem domain is in the hundreds of millions,
that is an unacceptable storage bound. The construction run time would also be
prohibitive given the magnitude of our input. \\
\\
We could use a range tree. The initial binary search tree would be sorted by
index and at each entry would be a pointer to a binary search tree sorted by
value. If we put the min value at each of the nodes of the initial tree, we can
speed up our queries. We would get a $O(log N)$ query time, but our
construction time and storage would be $O(N log N)$. Updating the data
structure would take at a minimum $O(k log(N))$ time if we do careful indexing
and only update the nodes that need to be updated. If we have a large access
requirement, then this would represent a significant improvement in query time.
Given our exceptionally large input, however, the construction, storage, and
update bounds are too prohibitive.  \\
\\
As it turns out, the common data structures that would be used for the finding the minimum value in an arbitrary part of a list are not practical for our purposes. Thus, while a simple linear search may seem inefficient at first, at it turns out it is the best option given our constraints. 
