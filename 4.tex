\section{Complexity Analysis}

We now analyze the running time and storage requirements of our algorithm. Let $N$ be the number of data units and $A$ be the number of access requirements. We will use $k$ as the average span of a single access requirement. The variable $Q$ will represent the number of data units that can be copied as specified by the user. Let $r$ be the amount of redundancy if we have a single-seek layout \cite{singleseeklayout}. The average number of overlapping access requirements for a single data unit ends up being $O(rN)$. 

{\bf Time Complexity:} The construction of the heaps $E_M$ and $E_C$ involve computing the benefit information for all $A$ access requirements and inserting each one into the heap. Computing the benefit information of moving or copying a data unit involves scanning the span of the access requirement which takes $O(k)$ operations, while inserting this benefit information into the heap takes $O(log (A))$ operations. Thus it takes a total of $O(A (k + logA))$ operations to do the initial construction of the heaps. \\
\\
After the initial construction, in every iteration, an element from the top of the heap is removed and processed, the benefit function is recalculated for affected access requirements, and the heap is updated. For each data unit move or copy, $O(rN)$ overlapping access requirements are affected, and for each of these access requirements $O(k+log(A))$ is required to recalculate the benefit data and update the heap. Thus each iteration takes $O(rN(k + logA))$ operations.\\
\\
For simplicity we will assume that the loop where we move the data units instead of copying is run $O(N)$ times total. This comes from the fact that the cache oblivious layout \cite{cacheobliviouslayout} should be a good approximation so the number of moves that would still be useful should be limited. We defined $Q$ as the number of redundant data units added. We thus can assert that there are $O(Q + N)$ iterations of the move or copy then update operation. This means that the moving and copying loops will take a total of $O((QrN + rN^2)(k + logA))$ operations. \\
\\
In total, our algorithm takes $O((QrN + rN^2 + A)(k + logA))$ operations. In practice, the optimal redundancy factors we found were greater than $2$ thus we can say that $Q > N$ meaning that $QrN > rN^2$. Additionally, since $r \geq 1$, we know that $rN > > A$. Thus we can simplify the expression to say that our algorithm takes $O(QrN(k + logA))$ operations. 

\subsection{Space Complexity}

During the run of the algorithm, we have to store the number of overlapping access requirements at each data unit, which will require $O(N)$ storage. We will also have to store a heap of access requirements, which can be stored using $O(A)$ space. We also have a list of access requirements and that information will take up $O(A)$ space. In total we thus have $O(A + N)$ storage space used during the run of the algorithm. 


\subsection{Linear search justification}

In order to find the best place to copy a data unit, we perform a linear search within the span of the access requirement for location with the largest benefit. If $k$ is the span of the access requirement, then the linear search takes $O(k)$ query time. Updates will also be $O(k)$ time. Construction of the list of data units where each data unit stores the number of overlapping access requirements will be $O(N)$. There are other approaches, such as a range tree or dynamic programming, that may produce better query times, but their construction and update times will be worse as well as their storage. \\
\\
With dynamic programming, we would have to maintain a matrix where an entry
$(i,j)$ would contain the minimum value in that range. This would give us a
$O(1)$ query time but the construction and storage would be $O(N^2)$ where N is
the number of data units. The update time would be $O(N)$ when we add a data
unit. Since the $N$ for this problem domain is in the hundreds of millions,
that is an unacceptable storage bound. The construction run time would also be
prohibitive given the magnitude of our input. \\
\\
We could use a range tree. The initial binary search tree would be sorted by
index and at each entry would be a pointer to a binary search tree sorted by
value. If we put the min value at each of the nodes of the initial tree, we can
speed up our queries. We would get a $O(log N)$ query time, but our
construction time and storage would be $O(N log N)$. Updating the data
structure would take at a minimum $O(k log(N))$ time if we do careful indexing
and only update the nodes that need to be updated. If we have a large access
requirement, then this would represent a significant improvement in query time.
Given our exceptionally large input, however, the construction, storage, and
update bounds are too prohibitive.  \\
\\
As it turns out, the common data structures that would be used for the finding the minimum value in an arbitrary part of a list are not practical for our purposes. Thus, while a simple linear search may seem inefficient at first, as it turns out it is the best option given our constraints. 
